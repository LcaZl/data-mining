{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import permutations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from datasketch import MinHash\n",
    "from tabulate import tabulate \n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot routes, matrices and dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_routes(df, key):\n",
    "    \"\"\"\n",
    "    Plots the 2D and 3D vectors from the DataFrame with all possible axis combinations in 3D, including centroids.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The DataFrame to plot.\n",
    "    key (str): The key in the DataFrame that contains the vectors to plot.\n",
    "    \"\"\"\n",
    "    pca2 = PCA(n_components=2)\n",
    "    pca3 = PCA(n_components=3)\n",
    "\n",
    "    vectors = df[key].tolist()\n",
    "    reduced_vectors_2 = pca2.fit_transform(vectors)\n",
    "\n",
    "    if len(vectors[0]) != 3:\n",
    "        vectors = pca3.fit_transform(vectors)\n",
    "\n",
    "    # Create 7 subplots in a 4x2 layout (4 rows, 2 columns)\n",
    "    fig = plt.figure(figsize=(20, 40))\n",
    "\n",
    "    # 2D plotting\n",
    "    ax2d = fig.add_subplot(4, 2, 1)  # 4 rows, 2 columns, position 1\n",
    "    ax2d.scatter([vec[0] for vec in reduced_vectors_2], [vec[1] for vec in reduced_vectors_2], cmap='Spectral', alpha=0.1)\n",
    "    ax2d.set_xlabel('PCA Component 1')\n",
    "    ax2d.set_ylabel('PCA Component 2')\n",
    "    ax2d.set_title(f'2D PCA Plot of {key}')\n",
    "\n",
    "    # 3D plotting for each axis combination\n",
    "    for i, (dim1, dim2, dim3) in enumerate(permutations(range(3), 3), start=3):\n",
    "        ax3d = fig.add_subplot(4, 2, i, projection='3d')\n",
    "        ax3d.scatter([vec[dim1] for vec in vectors], [vec[dim2] for vec in vectors], [vec[dim3] for vec in vectors], cmap='Spectral', depthshade=True, alpha=0.1)\n",
    "        ax3d.set_xlabel(f'Component {dim1+1}')\n",
    "        ax3d.set_ylabel(f'Component {dim2+1}')\n",
    "        ax3d.set_zlabel(f'Component {dim3+1}')\n",
    "        ax3d.set_title(f'3D Plot of Vectors ({dim1+1}, {dim2+1}, {dim3+1}) of {key}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_matrix(matrix, max_x=100, max_y=100, xlabel='X', ylabel='Y', title='No title'):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the given matrix using seaborn.\n",
    "\n",
    "    This function can plot the entire matrix or a specified part of it, depending on the max_x and max_y parameters.\n",
    "    It configures the aesthetics of the plot including the size, color map, line color, and linewidths.\n",
    "\n",
    "    Args:\n",
    "    matrix (2D array-like): The matrix to be plotted.\n",
    "    max_x (int, optional): The maximum number of rows to plot from the matrix. Defaults to 100.\n",
    "    max_y (int, optional): The maximum number of columns to plot from the matrix. Defaults to 100.\n",
    "    xlabel (str, optional): Label for the x-axis. Defaults to 'X'.\n",
    "    ylabel (str, optional): Label for the y-axis. Defaults to 'Y'.\n",
    "    title (str, optional): Title of the plot. Defaults to 'No title'.\n",
    "\n",
    "    Returns:\n",
    "    None: The function doesn't return anything; it only displays the plot.\n",
    "    \"\"\"\n",
    "    resized_matrix = matrix[:max_x, :max_y] if max_x and max_y else matrix\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    sns.heatmap(resized_matrix, annot=True, cmap='coolwarm', linecolor='white', linewidths=0.2)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def printTable(df, title, showIndex=True, limit=10):\n",
    "    \"\"\"\n",
    "    Prints a formatted table of the DataFrame to the console.\n",
    "\n",
    "    This function converts any NumPy arrays in the DataFrame to string format for better readability.\n",
    "    It uses the 'tabulate' library to format the table.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The DataFrame to be printed.\n",
    "    title (str): The title of the table.\n",
    "    showIndex (bool, optional): Flag to indicate if the index of the DataFrame should be shown. Defaults to True.\n",
    "    limit (int, optional): The maximum number of rows to print from the DataFrame. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    None: The function doesn't return anything; it only prints the table to the console.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    df_to_print = df.copy()\n",
    "    for col in df.columns:\n",
    "        if isinstance(df[col].iloc[0], np.ndarray):\n",
    "            df_to_print[col] = df[col].apply(lambda x: np.array2string(x))\n",
    "\n",
    "    print('\\n', title)\n",
    "    print(tabulate(df_to_print[:limit], headers='keys', showindex=showIndex, tablefmt='grid'))\n",
    "\n",
    "    \n",
    "\n",
    "def plot_vector_elements(df, key, cluster_column):\n",
    "    \"\"\"\n",
    "    Plots the 2D and 3D vectors from the DataFrame with all possible axis combinations in 3D, including centroids.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The DataFrame to plot.\n",
    "    key (str): The key in the DataFrame that contains the vectors to plot.\n",
    "    cluster_column (str): The column name in the DataFrame that contains the cluster labels.\n",
    "    \"\"\"\n",
    "    pca2 = PCA(n_components=2)\n",
    "    pca3 = PCA(n_components=3)\n",
    "\n",
    "    vectors = df[key].tolist()\n",
    "    reduced_vectors_2 = pca2.fit_transform(vectors)\n",
    "\n",
    "    if len(vectors[0]) != 3:\n",
    "        vectors = pca3.fit_transform(vectors)\n",
    "\n",
    "    # Create 7 subplots (1 for 2D and 6 for each axis combination in 3D)\n",
    "    fig = plt.figure(figsize=(12, 60))\n",
    "\n",
    "    # 2D plotting\n",
    "    ax2d = fig.add_subplot(711)  # 7 total rows, 1 column, position 1\n",
    "    ax2d.scatter([vec[0] for vec in reduced_vectors_2], [vec[1] for vec in reduced_vectors_2], c=df[cluster_column], cmap='Spectral', alpha=0.1)\n",
    "    ax2d.set_xlabel('PCA Component 1')\n",
    "    ax2d.set_ylabel('PCA Component 2')\n",
    "    ax2d.set_title('2D PCA Plot of Vectors')\n",
    "\n",
    "    # Calculate 2D centroids\n",
    "    for cluster_label in df[cluster_column].unique():\n",
    "        cluster_data_2d = reduced_vectors_2[df[cluster_column] == cluster_label]\n",
    "        centroid_2d = cluster_data_2d.mean(axis=0)\n",
    "        ax2d.scatter(*centroid_2d, color='red', marker='o', s=30, edgecolors='black', label='Centroid' if cluster_label == 0 else \"\")\n",
    "        \n",
    "    ax2d.legend()\n",
    "    # 3D plotting for each axis combination\n",
    "    for i, (dim1, dim2, dim3) in enumerate(permutations(range(3), 3), start=2):  # Start with subplot position 2\n",
    "        ax3d = fig.add_subplot(7, 1, i, projection='3d')\n",
    "        ax3d.scatter([vec[dim1] for vec in vectors], [vec[dim2] for vec in vectors], [vec[dim3] for vec in vectors], c=df[cluster_column], cmap='Spectral', depthshade=True, alpha=0.1)\n",
    "        ax3d.set_xlabel(f'Component {dim1+1}')\n",
    "        ax3d.set_ylabel(f'Component {dim2+1}')\n",
    "        ax3d.set_zlabel(f'Component {dim3+1}')\n",
    "        ax3d.set_title(f'3D Plot of Vectors ({dim1+1}, {dim2+1}, {dim3+1})')\n",
    "\n",
    "        # Calculate and plot 3D centroids for each axis combination\n",
    "        for cluster_label in df[cluster_column].unique():\n",
    "            cluster_data_3d = np.array([vectors[i] for i, label in enumerate(df[cluster_column]) if label == cluster_label])\n",
    "            centroid_3d = cluster_data_3d.mean(axis=0)\n",
    "            ax3d.scatter(centroid_3d[dim1], centroid_3d[dim2], centroid_3d[dim3], color='red', marker='o', s=30, edgecolors='black', label='Centroid' if cluster_label == 0 else \"\")\n",
    "        ax3d.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_driver_top_routes_from_json(json_filename, standard_route_df):\n",
    "    \"\"\"\n",
    "    Reads drivers data from a JSON file and prints each driver's top 5 routes.\n",
    "    Function to inspect the point 2 of the project tasks.\n",
    "\n",
    "    Args:\n",
    "        json_filename (str): Path to the JSON file containing drivers' data.\n",
    "        standard_route_df (pandas.DataFrame): DataFrame containing standard routes to match against.\n",
    "\n",
    "    The function reads the JSON file, iterates through each driver, and prints their routes, matching them with the standard routes DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(json_filename, 'r') as file:\n",
    "        drivers_data = json.load(file)\n",
    "\n",
    "    # Iterate through each driver in the JSON file\n",
    "    for driver_data in drivers_data:\n",
    "        print(f\"Driver {driver_data['driver']}:\")\n",
    "        for route in driver_data['routes']:\n",
    "\n",
    "            # Find the corresponding route in the standard routes DataFrame\n",
    "            matching_route = standard_route_df[standard_route_df['sroute_id'] == route]\n",
    "            if not matching_route.empty:\n",
    "\n",
    "                route_description = matching_route.iloc[0]['route']\n",
    "                print(f\"{route} - {route_description}\")\n",
    "            else:\n",
    "                print(f\"{route} - No matching route found\")\n",
    "\n",
    "    return drivers_data\n",
    "\n",
    "def plot_similarity(driver_values):\n",
    "\n",
    "    transposed_data = driver_values.T.values\n",
    "    names = driver_values.columns\n",
    "\n",
    "    for i, line in enumerate(transposed_data, start=0):\n",
    "        plt.plot(line, label=names[i])\n",
    "\n",
    "    plt.xlabel('Drivers')\n",
    "    plt.ylabel('Similarity (Non-divergence)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_routes(route_path, vocab, encoder, route_type):\n",
    "    \"\"\"\n",
    "    Loads routes from a file, processes them, and returns a DataFrame with encoded and additional route information.\n",
    "\n",
    "    Args:\n",
    "        route_path (str): Path to the JSON file containing route data.\n",
    "        vocab (Vocabulary object): An instance of the Vocabulary class for encoding purposes.\n",
    "        encoder (Encoder object): An encoder instance used for transforming route data.\n",
    "        route_type (str): Type of the routes ('actual' or other).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the processed and encoded route data.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Loads route data from a JSON file specified by `route_path`.\n",
    "    2. Creates a DataFrame from the loaded data.\n",
    "    3. Depending on the `route_type`, renames columns for clarity.\n",
    "    4. Generates sequences of cities and trips from the route data using the encoder.\n",
    "    5. Encodes the routes using the provided vocabulary.\n",
    "    6. Separates the encoded data into matrices and vectors, vectors are the flatten matrices.\n",
    "    7. Generates MinHash signatures for each route.\n",
    "    8. Returns the DataFrame with all the processed and encoded route information.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Loading {route_type} routes... ')\n",
    "    with open(route_path, 'r') as f:\n",
    "        routes_data = json.load(f)\n",
    "\n",
    "    routes_df = pd.DataFrame(routes_data)\n",
    "    \n",
    "    if route_type == \"actual\":\n",
    "        # Renaming columns for actual routes\n",
    "        routes_df.rename(columns={'id': 'aroute_id', 'sroute': 'sroute_id'}, inplace=True)\n",
    "    else:\n",
    "        # Renaming columns for other types of routes\n",
    "        routes_df.rename(columns={'id': 'sroute_id'}, inplace=True)\n",
    "    \n",
    "    # Generate city sequence for each route\n",
    "    routes_df['cities'] = routes_df['route'].apply(vocab.create_city_sequence)\n",
    "    # Generate trip sequence for each route\n",
    "    routes_df['trips'] = routes_df['route'].apply(vocab.create_trip_sequence)\n",
    "\n",
    "    # Encode routes using the provided vocabulary\n",
    "    routes_df['encoded_route'] = vocab.encode_route(routes_df)\n",
    "    print(f' -> Routes encoded')\n",
    "\n",
    "    # Separate encoded data into matrices and vectors\n",
    "    # Using zip and * to unpack matrix and vector returned by encode_m1 & encode_m2\n",
    "    routes_df['matrix'], routes_df['vector'] = zip(*routes_df['encoded_route'].apply(lambda r: encoder.vectorize(r)))\n",
    "    print(' -> Matrices & vectors generated.')\n",
    "\n",
    "    # Generate MinHash signatures for each route\n",
    "    routes_df['signature'] = routes_df['matrix'].apply(encoder.minhash_signature)\n",
    "    print(' -> Minhash signatures generated.')\n",
    "    \n",
    "    print(f'Loaded {route_type} routes.\\n')\n",
    "    return routes_df\n",
    "\n",
    "def load_driver_ideal_routes(route_path, vocabulary, encoder):\n",
    "    \"\"\"\n",
    "    Loads optimal route data from a file, processes them, and returns a DataFrame with encoded and additional route information.\n",
    "\n",
    "    Args:\n",
    "        route_path (str): Path to the JSON file containing route data.\n",
    "        vocabulary (Vocabulary object): An instance of the Vocabulary class for encoding purposes.\n",
    "        encoder (Encoder object): An encoder instance used for transforming route data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the processed and encoded route data.\n",
    "\n",
    "    The function performs the same steps of the above function and return a dataframe with the same structure.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Loading optimal routes... ')\n",
    "    with open(route_path, 'r') as f:\n",
    "        routes_data = json.load(f)\n",
    "\n",
    "    routes_df = pd.DataFrame(routes_data)\n",
    "\n",
    "    routes_df.rename(columns={'id': 'aroute_id'}, inplace=True)\n",
    "    \n",
    "    # Generate city sequence for each route\n",
    "    routes_df['cities'] = routes_df['route'].apply(encoder.create_city_sequence)\n",
    "    # Generate trip sequence for each route\n",
    "    routes_df['trips'] = routes_df['route'].apply(encoder.create_trip_sequence)\n",
    "\n",
    "    # Encode routes using the provided vocabulary\n",
    "    routes_df['encoded_route'] = vocabulary.encode_route(routes_df)\n",
    "    print(f' -> Routes encoded')\n",
    "\n",
    "    # Separate encoded data into matrices and vectors\n",
    "    # Using zip and * to unpack matrix and vector returned by encode_m1 & encode_m2\n",
    "    routes_df['matrix'], routes_df['vector'] = zip(*routes_df['encoded_route'].apply(lambda r: encoder.vectorize(r)))\n",
    "    print(' -> Matrices & vectors generated.')\n",
    "\n",
    "    # Generate MinHash signatures for each route\n",
    "    routes_df['signature'] = routes_df['matrix'].apply(encoder.minhash_signature)\n",
    "    print(' -> Minhash signatures generated.')\n",
    "    \n",
    "    print(f'Loaded optimal routes.\\n')\n",
    "\n",
    "    return routes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_top_routes_to_json(df, filename):\n",
    "    \"\"\"\n",
    "    Saves the top routes of drivers to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing drivers and their top routes.\n",
    "        filename (str): Name of the file to save the data.\n",
    "\n",
    "    The function iterates through each driver in the DataFrame, extracting their top 5 routes, and then saves this data to a JSON file.\n",
    "    \"\"\"\n",
    "    drivers_data = []\n",
    "\n",
    "    for driver, row in df.iterrows():\n",
    "        top_routes_df = row['top_routes']\n",
    "\n",
    "        # Extract top 5 routes from the nested DataFrame\n",
    "        top_routes = top_routes_df.index[:5].tolist()\n",
    "        drivers_data.append({'driver': driver, 'routes': top_routes})\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(drivers_data, file, indent=4)\n",
    " \n",
    "def save_standard_routes(df, column_name, file_path):\n",
    "    \"\"\"\n",
    "    Saves standard routes from a DataFrame to a JSON file in the standard format.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing route data.\n",
    "        column_name (str): Column name in the DataFrame that contains route IDs.\n",
    "        file_path (str): Path to save the JSON file.\n",
    "\n",
    "    The function creates a list of dictionaries from the DataFrame, where each dictionary contains route ID and details, and then saves it to a JSON file.\n",
    "    \"\"\"\n",
    "    data_to_save = [\n",
    "        {\"id\": row[column_name], \"route\": row[column_name]} \n",
    "        for index, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data_to_save, file, indent=2)\n",
    "\n",
    "def save_new_standard_routes(data_to_save, file_path):\n",
    "    \"\"\"\n",
    "    Saves new standard routes to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data_to_save (list of dicts): Data containing route information to be saved.\n",
    "        file_path (str): Path to the JSON file where the data will be saved.\n",
    "\n",
    "    The function writes the route data to a JSON file, using a custom converter for data types that are not natively serializable by JSON, such as numpy int32.\n",
    "    \"\"\"    \n",
    "    def default_converter(o):\n",
    "        if isinstance(o, np.int32):\n",
    "            return int(o)\n",
    "        if isinstance(o, np.int64):\n",
    "            return int(o)\n",
    "        raise TypeError(f'Object of type {o.__class__.__name__} is not JSON serializable')\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data_to_save, file, indent=2, default=default_converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The encoder class contains methods to:\n",
    "- Generate a vector from each route.\n",
    "- Extract the cities or trips sequence from a route\n",
    "- Apply PCA to a specified list of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, vocabulary, num_permutation):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder class with a vocabulary and a number of permutations for MinHash.\n",
    "\n",
    "        Args:\n",
    "            vocabulary (Vocabulary object): An instance of the Vocabulary class containing mappings for cities and merchandise.\n",
    "            num_permutation (int): The number of permutations to use for MinHash signatures.\n",
    "        \"\"\"\n",
    "        self.num_permutation = num_permutation\n",
    "        self.vocabulary = vocabulary\n",
    "        \n",
    "    def vectorize(self, encoded_route):\n",
    "        \"\"\"\n",
    "        Creates a matrix and vector representation of a route based on city and merchandise information.\n",
    "\n",
    "        Args:\n",
    "            encoded_route (list of tuples): Each tuple contains encoded city and merchandise data for a trip.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing a matrix representation of the route and a flattened vector form of the same.\n",
    "            \n",
    "        The function constructs a matrix where each row represents a trip. The row is a concatenation of a city vector and \n",
    "        a normalized merchandise vector. The city vector has an incremental value based on order at the indices representing the start and end cities of the trip, \n",
    "        and the merchandise vector has quantities for each merchandise type.\n",
    "        \"\"\"\n",
    "        cities_vector_length = len(self.vocabulary.city2id)\n",
    "        merch_vector_length = len(self.vocabulary.merchandise2id)\n",
    "        max_trip_count = self.vocabulary.max_trip\n",
    "\n",
    "        route_matrix = np.zeros((max_trip_count, cities_vector_length + merch_vector_length))\n",
    "        \n",
    "        for trip_order, (trip, merch) in enumerate(encoded_route):       \n",
    "\n",
    "            cities_vector = np.zeros(cities_vector_length)\n",
    "            cities_vector[trip[0]] = 2 * trip_order + 1\n",
    "            cities_vector[trip[1]] = 2 * trip_order + 2\n",
    "\n",
    "            merch_vector = np.zeros(merch_vector_length)\n",
    "            for merch_id, qty in merch.items():\n",
    "                merch_vector[merch_id] = qty\n",
    "\n",
    "            normalized_merch_vector = merch_vector / (self.vocabulary.max_trip * 2) # ---> Merchandise vector is normalized using the maximum quantity of merch in the dataset.\n",
    "            cities_merch_vector = np.concatenate([cities_vector, normalized_merch_vector])\n",
    "            \n",
    "            route_matrix[trip_order, :] = cities_merch_vector\n",
    "\n",
    "        route_vector = np.array(route_matrix.flatten())\n",
    "\n",
    "        return route_matrix, route_vector\n",
    "\n",
    "    def apply_PCA(self, df, key, n_components = 10, suffix = '_pca'):\n",
    "        \"\"\"\n",
    "        Applies Principal Component Analysis (PCA) to reduce the dimensionality of data.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): DataFrame containing the data.\n",
    "            key (str): Column name in the DataFrame which contains the data for PCA.\n",
    "            n_components (int, optional): Number of principal components to keep.\n",
    "            suffix (str, optional): Suffix to append to new PCA columns. Defaults to '_pca'.\n",
    "\n",
    "        This method reduces the dimensionality of the data in 'key' column of DataFrame 'df' using PCA and adds the \n",
    "        results as new columns in the DataFrame.\n",
    "        \"\"\"\n",
    "        vectors = df[key].tolist()\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_results = pca.fit_transform(vectors)\n",
    "\n",
    "        for i in range(n_components):\n",
    "            df[f'PCA{i}'] = np.array(pca_results[:, i])\n",
    "\n",
    "        df[f'{key}{suffix}'] = df.apply(lambda row: np.array([row[f'PCA{i}'] for i in range(n_components)]), axis=1)\n",
    "\n",
    "    def minhash_signature(self, vector):\n",
    "        \"\"\"\n",
    "        Computes the MinHash signature of a given vector.\n",
    "\n",
    "        Args:\n",
    "            vector (numpy.ndarray): The vector to compute the MinHash signature for.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of MinHash signatures.\n",
    "\n",
    "        This method computes a MinHash signature for the input vector using the specified number of permutations.\n",
    "        \"\"\"\n",
    "        minhash = MinHash(num_perm=self.num_permutation)\n",
    "        minhash.update(vector.tobytes())\n",
    "        return list(minhash.hashvalues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, standard_route_path, actual_route_path):\n",
    "        \"\"\"\n",
    "        Initializes the Vocabulary class by loading and processing route data from JSON files.\n",
    "\n",
    "        Args:\n",
    "            standard_route_path (str): Path to the JSON file containing standard routes.\n",
    "            actual_route_path (str): Path to the JSON file containing actual routes.\n",
    "\n",
    "        Attributes:\n",
    "            max_trip (int): The maximum number of trips in a route found in the data.\n",
    "            min_qty (float): The minimum quantity of any merchandise item across all routes.\n",
    "            max_qty (float): The maximum quantity of any merchandise item across all routes.\n",
    "            cities, merchandise, trips_city_pair, drivers, stdroutes, actroutes (set): Sets to store unique entities.\n",
    "            city2id, id2city, merchandise2id, id2merchandise, etc. (dict): Dictionaries for entity to ID mappings.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize maximum, minimum and data sets\n",
    "        self.max_trip = 0\n",
    "        self.min_qty = float('inf')\n",
    "        self.max_qty = 0\n",
    "\n",
    "        # Data sets\n",
    "        self.cities = set()\n",
    "        self.merchandise = set()\n",
    "        self.trips_city_pair = set()\n",
    "        self.drivers = set()\n",
    "        self.stdroutes = set()\n",
    "        self.actroutes = set()\n",
    "\n",
    "        # Process routing data from JSON files\n",
    "        self.process_routes(actual_route_path, self.actroutes)\n",
    "        self.process_routes(standard_route_path, self.stdroutes)\n",
    "\n",
    "        # Create vocabularies (mappings)\n",
    "        self.city2id, self.id2city = self.create_vocab(self.cities)\n",
    "        self.merchandise2id, self.id2merchandise = self.create_vocab(self.merchandise)\n",
    "        self.trip_city_pair2id, self.id2trip_city_pair = self.create_vocab(self.trips_city_pair)\n",
    "        self.driver2id, self.id2driver = self.create_vocab(self.drivers)\n",
    "\n",
    "        self.trips_set = set()\n",
    "        self.process_trips(actual_route_path)\n",
    "        self.process_trips(standard_route_path)\n",
    "\n",
    "        self.trip2id, self.id2trip = self.create_vocab(self.trips_set)\n",
    "        \n",
    "    def process_routes(self, file_path, routes):\n",
    "        \"\"\"\n",
    "        Processes routing data from a JSON file. Updates sets of cities, merchandise, trips, drivers, and routes.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the JSON file to be processed.\n",
    "            routes (set): A set to store unique route identifiers.\n",
    "\n",
    "        This method reads route data from the specified file and updates various sets with unique items, like cities and merchandise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = pd.DataFrame(json.load(f))\n",
    "\n",
    "            for _, row in data.iterrows():\n",
    "                routes.add(row['id'])\n",
    "\n",
    "                if 'driver' in row:\n",
    "                    self.drivers.add(row['driver'])\n",
    "\n",
    "                for i, trip in enumerate(row['route'], start=1):\n",
    "\n",
    "                    self.cities.update([trip['from'], trip['to']])\n",
    "                    self.trips_city_pair.add((trip['from'], trip['to']))\n",
    "\n",
    "                    for item, qty in trip['merchandise'].items():\n",
    "\n",
    "                        self.merchandise.add(item)\n",
    "                        self.min_qty = min(self.min_qty, qty)\n",
    "                        self.max_qty = max(self.max_qty, qty)\n",
    "\n",
    "                    self.max_trip = max(self.max_trip, i)\n",
    "        except IOError:\n",
    "            print(f\"Error reading file: {file_path}\")\n",
    "\n",
    "    def process_trips(self, file_path):\n",
    "        \"\"\"\n",
    "        Processes trip data from a JSON file. Updates the set of unique trips.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the JSON file to be processed.\n",
    "\n",
    "        This method reads trip data from each route of the specified file and updates the set of unique trips.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = pd.DataFrame(json.load(f))\n",
    "\n",
    "            for _, row in data.iterrows():\n",
    "                for _, trip in enumerate(row['route'], start=1):\n",
    "                    merch = []\n",
    "                    for item, qty in trip['merchandise'].items():\n",
    "                        merch.append((item, qty))\n",
    "\n",
    "                    element = [trip['from']] + [trip['to']] + merch\n",
    "                    self.trips_set.add(tuple(element))\n",
    "\n",
    "        except IOError:\n",
    "            print(f\"Error reading file: {file_path}\")\n",
    "\n",
    "    def create_vocab(self, data):\n",
    "        \"\"\"\n",
    "        Creates a vocabulary for a given set of data.\n",
    "\n",
    "        Args:\n",
    "            data (iterable): A collection of unique items.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Two dictionaries, one mapping items to IDs and the other mapping IDs back to items.\n",
    "\n",
    "        This method generates a vocabulary for encoding and decoding purposes.\n",
    "        \"\"\"\n",
    "        vocab = {el: i for i, el in enumerate(data)}\n",
    "        inverse_vocab = {i: el for el, i in vocab.items()}\n",
    "        return vocab, inverse_vocab\n",
    "    \n",
    "    def encode_route(self, dataframe):\n",
    "        \"\"\"\n",
    "        Encodes routes in a DataFrame using the established vocabularies.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): DataFrame containing route data.\n",
    "\n",
    "        Returns:\n",
    "            list: A list where each item is an encoded representation of a route.\n",
    "\n",
    "        This method encodes the routes in the DataFrame, converting city and merchandise information into their corresponding IDs.\n",
    "        \"\"\"\n",
    "        encoded_routes = []\n",
    "        for _, row in dataframe.iterrows():\n",
    "            route = row['route']\n",
    "            encoded_route = []\n",
    "\n",
    "            for trip in route:\n",
    "                start_city = trip['from']\n",
    "                end_city = trip['to']\n",
    "                merchandises = trip['merchandise']\n",
    "\n",
    "                encoded_merch = {self.merchandise2id[merch] : qty for merch, qty in merchandises.items()}\n",
    "                encoded_cities = (self.city2id.get(start_city), self.city2id.get(end_city))\n",
    "                \n",
    "                encoded_trip = (encoded_cities, encoded_merch)\n",
    "                encoded_route.append(encoded_trip)\n",
    "\n",
    "            encoded_routes.append(encoded_route)\n",
    "\n",
    "        return encoded_routes\n",
    "\n",
    "    def create_city_sequence(self, route):\n",
    "        \"\"\"\n",
    "        Converts a route into a sequence of cities.\n",
    "\n",
    "        Args:\n",
    "            route (list of dicts): A route represented as a list of dictionaries, each dict containing 'from' and 'to' keys.\n",
    "\n",
    "        Returns:\n",
    "            list: A sequential list of cities in the order they are visited in the route.\n",
    "\n",
    "        This method creates a list of cities visited in a route based on the 'from' and 'to' keys of each trip in the input route.\n",
    "        \"\"\"\n",
    "        taken_route = []\n",
    "        for path in route:\n",
    "            if taken_route:\n",
    "                taken_route.append(path['to'])\n",
    "            else:\n",
    "                taken_route.append(path['from'])\n",
    "                taken_route.append(path['to'])\n",
    "        return taken_route\n",
    "\n",
    "    def create_trip_sequence(self, route):\n",
    "        \"\"\"\n",
    "        Converts a route into a sequence of trips in a hashable format. Each trip is represented as a tuple.\n",
    "\n",
    "        Args:\n",
    "            route (list of dicts): A route\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tuples, each representing a trip in the routeù\n",
    "\n",
    "        This method iterates over each trip in the provided route. For each trip, it constructs a tuple that starts with \n",
    "        the origin ('from') and destination ('to') cities, followed by a list of (merchandise, quantity) pairs. This tuple \n",
    "        format encapsulates all the relevant information for a trip in a single, hashable data structure.\n",
    "        \"\"\"\n",
    "        trips = []\n",
    "        for trip in route:\n",
    "            trips.append(tuple(\n",
    "                [trip['from']] + \n",
    "                [trip['to']] +\n",
    "                [(merch, qty) for merch, qty in trip['merchandise'].items()]\n",
    "            ))\n",
    "        return trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity matrices generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_matrix(standard_routes_df, actual_routes_df, key_field, similarity, save_path):\n",
    "    \"\"\"\n",
    "    Computes a similarity matrix between standard and actual routes based on the specified similarity measure.\n",
    "\n",
    "    Args:\n",
    "        standard_routes_df (pandas.DataFrame): DataFrame containing standard route data.\n",
    "        actual_routes_df (pandas.DataFrame): DataFrame containing actual route data.\n",
    "        key_field (str): Field name in both DataFrames used for similarity calculation.\n",
    "        similarity (str): Type of similarity measure ('jaccard', 'cosine', or 'euclidean').\n",
    "        save_path (str): Path to save the calculated similarity matrix.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A matrix where each element represents the similarity score between a pair of standard and actual routes.\n",
    "\n",
    "    The function computes pairwise similarities for each actual route against all standard routes using the specified similarity measure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load an existing similarity matrix if available\n",
    "    similarity_matrix = load_similarity_matrix(save_path)\n",
    "    if similarity_matrix is not None:\n",
    "        print(f'Matrix loaded from file {save_path}.')\n",
    "        return similarity_matrix\n",
    "\n",
    "    # Initialize an empty similarity matrix\n",
    "    num_actual_routes = len(actual_routes_df)\n",
    "    num_standard_routes = len(standard_routes_df)\n",
    "    similarity_matrix = np.zeros((num_actual_routes, num_standard_routes))\n",
    "\n",
    "    # Compute the similarity for each pair of actual and standard routes\n",
    "    for i, actual_row in tqdm(enumerate(actual_routes_df.itertuples()), total=num_actual_routes, desc='Building similarity matrix...'):\n",
    "        for j, standard_row in enumerate(standard_routes_df.itertuples()):\n",
    "            # Calculate similarity based on the chosen method\n",
    "            if similarity == 'jaccard':\n",
    "                similarity_matrix[i, j] = jaccard_similarity(getattr(actual_row, key_field), getattr(standard_row, key_field))\n",
    "            elif similarity == 'cosine':\n",
    "                similarity_matrix[i, j] = cosine_similarity(getattr(actual_row, key_field).reshape(1, -1), getattr(standard_row, key_field).reshape(1, -1))[0,0]\n",
    "\n",
    "    # Save the calculated similarity matrix\n",
    "    np.save(save_path, similarity_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "def jaccard_similarity(signature1, signature2):\n",
    "    \"\"\"\n",
    "    Computes the Jaccard similarity between two signatures.\n",
    "\n",
    "    Args:\n",
    "        signature1 (numpy.ndarray): The first signature array.\n",
    "        signature2 (numpy.ndarray): The second signature array.\n",
    "\n",
    "    Returns:\n",
    "        float: The Jaccard similarity score between the two signatures.\n",
    "\n",
    "    The Jaccard similarity is calculated as the sum of the minimum intersection divided by the sum of the maximum union of the two signatures.\n",
    "    \"\"\"\n",
    "    min_intersection = np.minimum(signature1, signature2).sum()\n",
    "    max_union = np.maximum(signature1, signature2).sum()\n",
    "\n",
    "    # Handle division by zero\n",
    "    if max_union == 0:\n",
    "        return 0\n",
    "\n",
    "    return min_intersection / max_union\n",
    "\n",
    "\n",
    "def euclidean_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Calculates Euclidean similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "        vector1 (numpy.ndarray): The first vector.\n",
    "        vector2 (numpy.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "        float: The Euclidean similarity score between the two vectors.\n",
    "\n",
    "    The Euclidean similarity is computed as an exponentially decreasing function of the Euclidean distance between the vectors.\n",
    "    \"\"\"\n",
    "    # Compute the Euclidean distance\n",
    "    distance = np.linalg.norm(vector1 - vector2)\n",
    "\n",
    "    # Convert distance to similarity in the range [0, 1]\n",
    "    similarity = np.exp(-distance)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def euclidean_similarity_matrix(X):\n",
    "    \"\"\"\n",
    "    Calculates similarity based on Euclidean distance between rows of a matrix.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): An input matrix where each row represents an item (e.g., a trip or a driver).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A similarity matrix where the element (i, j) represents the similarity between the i-th and j-th rows of X.\n",
    "\n",
    "    This method computes the Euclidean distance between every pair of rows in X.\n",
    "    To convert it into a measure of similarity, we use the formula 1 / (1 + distance). 1 is added to the denominator to avoid division by zero.\n",
    "    \"\"\"\n",
    "    distances = euclidean_distances(X)\n",
    "    # Transforming the distance into a measure of similarity\n",
    "    similarity = 1 / (1 + distances)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def load_similarity_matrix(save_path):\n",
    "    \"\"\"\n",
    "    Attempts to load a pre-existing similarity matrix from a specified file path.\n",
    "\n",
    "    Args:\n",
    "        save_path (str): Path where the similarity matrix file is expected to be located.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray or None: The similarity matrix if the file exists and is successfully loaded, otherwise None.\n",
    "\n",
    "    This function is used to check for and load an existing similarity matrix, which can save computation time if the matrix has been previously calculated.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the matrix from the specified path\n",
    "        similarity_matrix = np.load(save_path)\n",
    "        return similarity_matrix\n",
    "    except FileNotFoundError:\n",
    "        # Return None if the file does not exist\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtilityMatrix:\n",
    "    \n",
    "    def __init__(self, routes, comparison_key, vocabulary):\n",
    "        \"\"\"\n",
    "        Initializes the UtilityMatrix class for creating a utility matrix from route data.\n",
    "\n",
    "        Args:\n",
    "            comparison_key (str): Key used for comparison in generating the utility matrix.\n",
    "            vocabulary (Vocabulary object): An instance of the Vocabulary class.\n",
    "            metric (str): label used to select the similarity metric to use for all the computations.\n",
    "\n",
    "        Attributes:\n",
    "            matrix_lv1, trip_count_matrix, trip_sum_matrix (numpy.ndarray): Matrices representing different levels of utility based on the routes.\n",
    "            trip_count_matrix_norm (numpy.ndarray): Normalized trip count matrix.\n",
    "\n",
    "        The class calculates utility matrices based on driver and trip data, using the provided vocabulary for encoding.\n",
    "        \"\"\"\n",
    "        self.vocabulary = vocabulary\n",
    "        self.key = comparison_key\n",
    "        self.matrix_lv1, self.trip_count_matrix, self.trip_sum_matrix = self.driver_x_trip_matrix(routes)\n",
    "\n",
    "    def driver_x_trip_matrix(self, routes):\n",
    "        \"\"\"\n",
    "        Generates a utility matrix representing the relationship between drivers and trips.\n",
    "\n",
    "        Args:\n",
    "            routes (pandas.DataFrame): DataFrame containing route data.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the computed utility matrix, trip count matrix, and trip sum matrix.\n",
    "\n",
    "        This method calculates matrices that map the relationship between drivers and trips based on the trips data. \n",
    "        It uses th specified metric to measure the similarity between each actual trip of the actual route and each standard trip of the correspondant standard route.\n",
    "        \"\"\"\n",
    "        # Initialize matrices for sum and count\n",
    "        sum_matrix = np.zeros((len(self.vocabulary.driver2id), len(self.vocabulary.trip2id)))\n",
    "        count_matrix = np.zeros((len(self.vocabulary.driver2id), len(self.vocabulary.trip2id)))\n",
    "\n",
    "        # Calculate the length of encoded city and merchandise vectors\n",
    "        city_encoded_length = len(self.vocabulary.city2id)\n",
    "        merch_encoded_length = len(self.vocabulary.merchandise2id)\n",
    "        trip_vector_len = city_encoded_length + merch_encoded_length       \n",
    "\n",
    "        # Iterate through routes to populate matrices\n",
    "        for _, row in tqdm(routes.iterrows(), total=len(routes), desc='Generating level 1 utility matrix'):\n",
    "            driver_idx = self.vocabulary.driver2id.get(row['driver'])\n",
    "            std_trips = row['trips_std']\n",
    "            act_trips = row['trips_act']\n",
    "            std_vector = row[f'{self.key}_std']\n",
    "            act_vector = row[f'{self.key}_act']\n",
    "        \n",
    "            for i, act_trip_value in enumerate(act_trips):\n",
    "                act_trip_index = self.vocabulary.trip2id.get(act_trip_value)\n",
    "                cumulated_similarity = 0\n",
    "                comparison_count = 0\n",
    "\n",
    "                # Compare actual trip with standard trips using cosine similarity\n",
    "                for k in range(len(std_trips)):\n",
    "\n",
    "                    std_vector_slice = std_vector[k * trip_vector_len : (k+1) * trip_vector_len]\n",
    "                    act_vector_slice = act_vector[i * trip_vector_len : (i+1) * trip_vector_len]\n",
    "\n",
    "                    similarity_increase = cosine_similarity(act_vector_slice.reshape(1, -1), std_vector_slice.reshape(1, -1))[0, 0]\n",
    "\n",
    "                    # Special handling when actual trip and standard trip are the same\n",
    "                    if similarity_increase == 1 and i == k:\n",
    "                        cumulated_similarity = 1\n",
    "                        comparison_count = 1\n",
    "                        break\n",
    "\n",
    "                    cumulated_similarity += similarity_increase\n",
    "                    comparison_count += 1\n",
    "\n",
    "                # Update the sum and count matrices\n",
    "                sum_matrix[driver_idx, act_trip_index] += cumulated_similarity\n",
    "                count_matrix[driver_idx, act_trip_index] += comparison_count\n",
    "\n",
    "        # Compute and return the utility matrices\n",
    "        mask = count_matrix != 0\n",
    "        um = np.zeros_like(count_matrix, dtype=float)\n",
    "        um[mask] = np.round(sum_matrix[mask] / count_matrix[mask], 5)\n",
    "\n",
    "        # Normalize the trip count matrix. This step is essential to compare trips on a similar scale later,\n",
    "        # especially when the number of trips can vary significantly. Normalization is done by dividing \n",
    "        # each element in the trip count matrix by the maximum value in this matrix, resulting in a matrix \n",
    "        # where all count values are between 0 and 1.\n",
    "\n",
    "        count_matrix = count_matrix / np.max(count_matrix)\n",
    "        return um, count_matrix, sum_matrix\n",
    "\n",
    "\n",
    "    def predict_ratings_with_driver_profile_top_k(self, driver_profiles, top_k_drivers, precision=5):\n",
    "        \"\"\"\n",
    "        Predicts utility matrix ratings using driver profile similarities for the top-k similar drivers.\n",
    "\n",
    "        Args:\n",
    "            driver_profiles (numpy.ndarray): Matrix containing driver profile similarities.\n",
    "            top_k_drivers (list): List of top-k similar drivers for each driver.\n",
    "            precision (int): Decimal precision for the predicted ratings.\n",
    "\n",
    "        Updates the matrix_lv2_driver attribute with the predicted ratings.\n",
    "        \"\"\"\n",
    "        num_drivers, num_trips = self.matrix_lv1.shape\n",
    "        predicted_matrix = self.matrix_lv1.copy()\n",
    "\n",
    "        for driver_A in tqdm(range(num_drivers), total=num_drivers, desc='Predicting ratings with driver profiles'):\n",
    "            # Use top-k similar drivers for prediction\n",
    "            similar_drivers = top_k_drivers[driver_A]\n",
    "\n",
    "            for trip in range(num_trips):\n",
    "                if predicted_matrix[driver_A, trip] == 0:\n",
    "                    # Calculate the weighted average rating\n",
    "                    total_rating = sum(driver_profiles[driver_A, driver_B] * self.matrix_lv1[driver_B, trip] for driver_B in similar_drivers if self.matrix_lv1[driver_B, trip] > 0)\n",
    "                    total_similarity = sum(driver_profiles[driver_A, driver_B] for driver_B in similar_drivers if self.matrix_lv1[driver_B, trip] > 0)\n",
    "\n",
    "                    if total_similarity > 0:\n",
    "                        predicted_rating = total_rating / total_similarity\n",
    "                        predicted_matrix[driver_A, trip] = np.round(predicted_rating, precision)\n",
    "\n",
    "        self.matrix_lv2 = predicted_matrix\n",
    "\n",
    "        \n",
    "def get_top_k_similar_indices(similarity_matrix, k):\n",
    "    \"\"\"\n",
    "    Identifies the top-k similar indices for each row in a similarity matrix.\n",
    "    Used with drivers and trips profiles.\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (numpy.ndarray): A square matrix where each element represents a similarity score.\n",
    "        k (int): The number of top similar elements to identify for each row.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where each key is a row index and the value is an array of indices of the top-k similar elements.\n",
    "\n",
    "    This function computes the top-k similar indices for each row in the similarity matrix.\n",
    "    It first converts the similarity matrix into a sparse matrix for efficient computation, then iterates over each row to find the top-k similar elements.\n",
    "    \"\"\"\n",
    "    # Convert to sparse matrix for efficient computation\n",
    "    sparse_sim = csr_matrix(similarity_matrix)\n",
    "    top_k_indices = {}\n",
    "\n",
    "    for index in tqdm(range(sparse_sim.shape[0]), total=sparse_sim.shape[0], desc=f'Computing top-{k} similarities'):\n",
    "        row = sparse_sim[index]\n",
    "        \n",
    "        if row.nnz > k:\n",
    "            # Indices of top-k highest values in the row\n",
    "            top_indices = np.argsort(row.data)[-k:][::-1]\n",
    "            top_k_indices[index] = row.indices[top_indices]\n",
    "        else:\n",
    "            # All indices if the non-zero elements are fewer than k\n",
    "            top_k_indices[index] = row.indices\n",
    "\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriversProfile:\n",
    "    def __init__(self, utility_matrix, vocabulary):\n",
    "        \"\"\"\n",
    "        Initializes the profiles class.\n",
    "\n",
    "        Args:\n",
    "            utility_matrix (numpy.ndarray): The utility matrix used for creating profiles.\n",
    "            vocabulary (Vocabulary object): The vocabulary used in the utility matrix.\n",
    "            metric (str): The metric to use to compute the similarity.\n",
    "        Attributes:\n",
    "            utility_matrix (numpy.ndarray): Stored utility matrix.\n",
    "            vocabulary (Vocabulary object): Stored vocabulary object.\n",
    "        \"\"\"\n",
    "        self.utility_matrix = utility_matrix\n",
    "        self.vocabulary = vocabulary\n",
    "        self.generate_driver_profiles(precision=5)\n",
    "\n",
    "    def generate_driver_profiles(self, precision=5):\n",
    "        \"\"\"\n",
    "        Generates cosine or Euclidean similarity profiles for each driver based on a reduced dimensionality\n",
    "        of the utility matrix, achieved through PCA, reducing the feature space to 3 components.\n",
    "\n",
    "        Args:\n",
    "            precision (int): The decimal precision to use for the similarity scores.\n",
    "\n",
    "        Updates:\n",
    "            self.drivers_profile (numpy.ndarray): A matrix containing the cosine similarity scores between all pairs of drivers,\n",
    "            post-dimensionality reduction.\n",
    "\n",
    "        This method applies PCA on the utility matrix to reduce its dimensionality to 3 components before calculating\n",
    "        the similarity. The resulting drivers_profile matrix provides a measure of similarity\n",
    "        between drivers, indicating how similar two drivers are in terms of their utility patterns in this reduced feature space.\n",
    "        \"\"\"\n",
    "        # Apply PCA to reduce dimensions to 3\n",
    "        pca = PCA(n_components=10)\n",
    "        reduced_matrix = pca.fit_transform(self.utility_matrix)\n",
    "\n",
    "        # Calculate similarity between drivers based on the selected metric\n",
    "        self.drivers_profile = np.round(cosine_similarity(reduced_matrix), precision)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trip_evaluation(utility_matrix, weights, vocabulary):\n",
    "    \"\"\"\n",
    "    Evaluates trips based on a utility matrix and weights, and ranks them accordingly.\n",
    "\n",
    "    Args:\n",
    "        utility_matrix (numpy.ndarray): The utility matrix representing trips.\n",
    "        weights (numpy.ndarray): The weights associated with each trip.\n",
    "        vocabulary (Vocabulary object): The vocabulary object containing trip IDs and other details.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing trip evaluation scores and additional information.\n",
    "\n",
    "    This function computes various metrics for trips such as average score, count, and normalized scores. These metrics are used to rank the trips.\n",
    "    \"\"\"\n",
    "    # Calculate the sum of non-zero elements in each column\n",
    "    sum_non_zero_trips = np.where(utility_matrix != 0, utility_matrix, 0).sum(axis=0)\n",
    "    \n",
    "    # Count the number of non-zero values per column\n",
    "    count_non_zero_trips = (utility_matrix != 0).sum(axis=0)\n",
    "\n",
    "    # Calculate the average, avoiding division by zero\n",
    "    trip_scores = np.divide(sum_non_zero_trips, count_non_zero_trips, out=np.zeros_like(sum_non_zero_trips))\n",
    "\n",
    "    # Calculate the total counts and normalize them\n",
    "    counts = weights.sum(axis=0)  # Get the total number of times that a trip has been executed\n",
    "    normalized_counts = counts / np.max(counts)\n",
    "    score = trip_scores * normalized_counts\n",
    "    norm_score = score / score.max()\n",
    "\n",
    "    # Create a DataFrame with trip evaluation details\n",
    "    df = pd.DataFrame({\n",
    "        'trip_id': range(len(vocabulary.trip2id)),\n",
    "        'trip': [vocabulary.id2trip.get(i) for i in range(len(vocabulary.trip2id))],\n",
    "        'trip_liked_avg_score': trip_scores,\n",
    "        'count': counts,\n",
    "        'normalized_count': normalized_counts,\n",
    "        'score': score,\n",
    "        'normalized_score' : norm_score\n",
    "    })\n",
    "    df.set_index('trip_id', inplace=True)\n",
    "    return df.sort_values(by='normalized_score', ascending=False)\n",
    "\n",
    "\n",
    "def add_trip_level_feature(trip_ranking, routes_df, vocabulary, feature_name):\n",
    "    \"\"\"\n",
    "    Adds a trip-level feature to a DataFrame based on the trip rankings.\n",
    "\n",
    "    Args:\n",
    "        trip_ranking (pandas.DataFrame): DataFrame containing trip rankings and scores.\n",
    "        routes_df (pandas.DataFrame): DataFrame to which the trip feature is to be added.\n",
    "        vocabulary (Vocabulary object): The vocabulary object containing trip IDs and other details.\n",
    "        feature_name (str): Name of the feature to be added.\n",
    "\n",
    "    The function iterates through each row in the routes DataFrame, adds a new trip level feature based on the trip rankings, \n",
    "    and updates the DataFrame with new matrices and vectors with the correspondant feature value incorporated.\n",
    "    \"\"\"\n",
    "    # New feature fields.\n",
    "    routes_df['matrix_f1'] = None\n",
    "    routes_df['vector_f1'] = None\n",
    "\n",
    "    # Insepct each row of the dataframe\n",
    "    for index, row in tqdm(routes_df.iterrows(), total = len(routes_df), desc = 'Adding trip rating feature'):\n",
    "\n",
    "\n",
    "        preferences_features = np.zeros((vocabulary.max_trip, 1)) # Column of ne features to add to the old matrix\n",
    "        trips = [vocabulary.trip2id.get(trip) for trip in row['trips']] # Get trip ids of the current row (route)\n",
    "\n",
    "        for i, trip_id in enumerate(trips): # For each trip\n",
    "\n",
    "            # Get the score from the input dataframe\n",
    "            trip_score = trip_ranking.loc[trip_id, feature_name]\n",
    "\n",
    "            # Set the score as feature in the column\n",
    "            preferences_features[i, 0] = trip_score\n",
    "\n",
    "        # Check shape\n",
    "        assert row['matrix'].shape[0] == preferences_features.shape[0]\n",
    "\n",
    "        # Add the column to the specified key field of the row\n",
    "        combined_matrix = np.hstack((row['matrix'], preferences_features))\n",
    "        \n",
    "        # Insert the new objects into the original dataframe as new fields\n",
    "\n",
    "        routes_df.at[index, 'matrix_f1'] = combined_matrix\n",
    "        routes_df.at[index, 'vector_f1'] = combined_matrix.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_evaluate(data, params, key, approach='kmeans'):\n",
    "    \"\"\"\n",
    "    Performs clustering on the dataset and evaluates the clustering performance.\n",
    "\n",
    "    This function applies a clustering algorithm (either KMeans or KMedoids) to the dataset based on the specified parameters and key. \n",
    "    It calculates and returns the Silhouette and Davies-Bouldin scores for the clustering, which are metrics used to evaluate the quality of the clustering.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the data to cluster.\n",
    "        params (dict): Parameters for the clustering algorithm.\n",
    "        key (str): The key in the DataFrame that contains the vectors to use for clustering.\n",
    "        approach (str): The clustering approach to use ('kmeans' or 'kmedoids').\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the clustering model, Silhouette score, and Davies-Bouldin score.\n",
    "    \"\"\"\n",
    "    # Select the clustering model based on the specified approach\n",
    "    if approach == 'kmeans':\n",
    "        model = KMeans(**params, random_state=1)\n",
    "    else:\n",
    "        model = KMedoids(**params, random_state=1,)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    model.fit(data[key].tolist())\n",
    "    # Assign cluster labels to the data\n",
    "    data[f'cluster_{approach}'] = model.labels_\n",
    "\n",
    "    # Calculate clustering evaluation metrics\n",
    "    silhouette = silhouette_score(data[key].tolist(), model.labels_, metric='cosine')\n",
    "    bouldin = davies_bouldin_score(data[key].tolist(), model.labels_)\n",
    "\n",
    "    return model, silhouette, bouldin\n",
    "\n",
    "def clustering_summary(df, cluster_column):\n",
    "    \"\"\"\n",
    "    Generates a summary of each cluster.\n",
    "\n",
    "    This function calculates the number of elements in each cluster and compiles a list of route IDs\n",
    "    that belong to each cluster. It also plots a histogram showing the distribution of the clusters.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing cluster labels and route IDs.\n",
    "        cluster_column (str): The column name in the DataFrame that contains the cluster labels.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing a summary of each cluster, including the number of elements and the route IDs.\n",
    "    \"\"\"\n",
    "    cluster_summary = []\n",
    "\n",
    "    # Iterate through each unique cluster label\n",
    "    for cluster_label in df[cluster_column].unique():\n",
    "        cluster_items = df[df[cluster_column] == cluster_label]\n",
    "\n",
    "        # Count the number of items and gather route IDs in the cluster\n",
    "        num_items = len(cluster_items)\n",
    "        route_ids = cluster_items['aroute_id'].tolist()  # or 'sroute_id' for standard routes\n",
    "\n",
    "        # Append the cluster summary information\n",
    "        cluster_summary.append({\n",
    "            'cluster_label': cluster_label,\n",
    "            'num_elements': num_items,\n",
    "            'route_ids': route_ids,\n",
    "        })\n",
    "\n",
    "    # Convert the summary to a DataFrame\n",
    "    summary_df = pd.DataFrame(cluster_summary)\n",
    "    \n",
    "    # Plot a histogram to visualize the distribution of clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    df[cluster_column].value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
    "    plt.xlabel('Cluster Label')\n",
    "    plt.ylabel('Number of Elements')\n",
    "    plt.title(f'Distribution of Clusters - {cluster_column}')\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()  # Ensures everything fits in the plot without overlapping\n",
    "    plt.show()\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "def find_optimal_clusters(data, key, approach, file_path, k_min=2, k_max=10, step=1):\n",
    "    \"\"\"\n",
    "    Finds the optimal number of clusters by evaluating clustering performance across a range of cluster counts.\n",
    "\n",
    "    This function iteratively applies a clustering algorithm (either KMeans or KMedoids) for different values of k (number of clusters).\n",
    "    For each k, it calculates the sum of squared distances (SSE), Silhouette scores, and Davies-Bouldin scores. The results are then\n",
    "    plotted to help identify the optimal number of clusters (the 'elbow point' in the SSE plot and optimal points in Silhouette and Davies-Bouldin score plots).\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the data to be clustered.\n",
    "        key (str): The key in the DataFrame that holds the data for clustering.\n",
    "        approach (str): The clustering approach to use ('kmeans' or 'kmedoids').\n",
    "        k_min (int): The minimum number of clusters to test.\n",
    "        k_max (int): The maximum number of clusters to test.\n",
    "        step (int): The step size to use when iterating over the range of cluster counts.\n",
    "    \"\"\"\n",
    "    sse = []  # Sum of squared distances\n",
    "    silhouette_scores = []\n",
    "    bouldin_scores = []\n",
    "    aggregated_scores = []\n",
    "    k_values = range(k_min, k_max + 1, step)\n",
    "\n",
    "    for k in tqdm(k_values):\n",
    "        if approach == 'kmeans':\n",
    "            parameters = {'n_clusters':k, 'init':'k-means++', 'n_init':'auto'}\n",
    "        else:\n",
    "            parameters = {'n_clusters': k, 'init': 'k-medoids++', 'metric':'cosine'}\n",
    "        model, silhouette, davies_bouldin = cluster_and_evaluate(data, parameters, key, approach)\n",
    "\n",
    "        # Calculate the sum of squared distances\n",
    "        sse.append(model.inertia_)\n",
    "        \n",
    "        # Calculate the Silhouette score and Davies-Bouldin score\n",
    "        silhouette_scores.append(silhouette)\n",
    "        bouldin_scores.append(davies_bouldin)\n",
    "        aggregated_scores.append(np.max(davies_bouldin - silhouette, 0))\n",
    "\n",
    "    # Plotting the elbow curve and the scores\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.title(f'{key}_{approach}_{k_min}{k_max}{step}')\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(k_values, sse, '-o')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Sum of squared distances (SSE)')\n",
    "    plt.title('Elbow Curve for Inertia')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(k_values, silhouette_scores, '-o')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score Curve')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(k_values, bouldin_scores, '-o')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Davies-Bouldin Score')\n",
    "    plt.title('Davies-Bouldin Score Curve')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(k_values, aggregated_scores, '-o')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Aggregated Score')\n",
    "    plt.title('Aggregated Score Curve')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if file_path:\n",
    "        plt.savefig(file_path)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def grid_search(parameter_combinations, data, key, approach, plot_enabled=True):\n",
    "    \"\"\"\n",
    "    Performs a grid search to find the best clustering parameters based on evaluation metrics.\n",
    "\n",
    "    This function iterates over a set of parameter combinations for a clustering algorithm, evaluating each combination\n",
    "    using the cluster_and_evaluate function. It determines the best parameter set based on the Silhouette and Davies-Bouldin scores.\n",
    "    The best configuration is determined based on the highest aggregated score, which is a combination of normalized Silhouette and Davies-Bouldin scores.\n",
    "\n",
    "    Args:\n",
    "        parameter_combinations (list of dicts): A list of dictionaries, each representing a set of clustering parameters.\n",
    "        data (pandas.DataFrame): The DataFrame containing the data to cluster.\n",
    "        key (str): The key in the DataFrame that contains the data to use for clustering.\n",
    "        approach (str): The clustering approach to use ('kmeans' or 'kmedoids').\n",
    "        plot_enabled (bool): If True, plots the clustering summary and vector elements for each parameter set.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the best aggregated score and the corresponding best parameter configuration.\n",
    "\n",
    "    \"\"\"\n",
    "    best_score = (0, 0, 0)  # Initialize the best score\n",
    "    best_config = None  # Initialize the best configuration\n",
    "\n",
    "    # Iterate over parameter combinations and evaluate each\n",
    "    pbar = tqdm(enumerate(parameter_combinations), total=len(parameter_combinations))\n",
    "    for i, params in pbar:\n",
    "        pbar.set_description(f'Best score: {round(best_score[0],3)} - S:{round(best_score[1],3)} - B:{round(best_score[2],3)}')\n",
    "        print(f'\\nConfiguration {i+1}/{len(parameter_combinations)}')\n",
    "        _, silhouette, davies_bouldin = cluster_and_evaluate(data, params, key, approach)\n",
    "\n",
    "        # Normalize the Silhouette and Davies-Bouldin scores for combination\n",
    "        normalized_silhouette = (silhouette + 1) / 2  # Normalize between 0 and 1\n",
    "        normalized_davies_bouldin = 1 / (1 + davies_bouldin)  # Higher is better\n",
    "\n",
    "        # Combine scores to determine the overall performance of the configuration\n",
    "        aggregated_score = normalized_silhouette + normalized_davies_bouldin\n",
    "\n",
    "        print(f'Parameters: {params}')\n",
    "        print(f'Silhouette score: {silhouette}')\n",
    "        print(f'Davies-Bouldin score: {davies_bouldin}')\n",
    "\n",
    "        # Plot the clustering summary and vector elements if enabled\n",
    "        if plot_enabled:\n",
    "            clustering_summary(data, f'cluster_{approach}')\n",
    "            plot_vector_elements(data, key, f'cluster_{approach}')\n",
    "\n",
    "        # Update the best score and configuration if the current one is better\n",
    "        if aggregated_score > best_score[0]:\n",
    "            best_score = (aggregated_score, silhouette, davies_bouldin)\n",
    "            best_config = params\n",
    "    \n",
    "    return (best_score, best_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routes generator with clustering - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_values(values, vocabulary):\n",
    "    \"\"\"\n",
    "    Transforms and normalizes an array of values based on a given vocabulary's maximum quantity (max_qty).\n",
    "\n",
    "    The function performs three main operations:\n",
    "    1. Re-normalizes the values by scaling them according to the 'max_qty' from the vocabulary.\n",
    "    2. Applies a threshold by setting any value less than 1 to zero, filtering out negligible values.\n",
    "    3. Rounds the values to the nearest integer, converting them into integer format.\n",
    "\n",
    "    Args:\n",
    "        values (array): Array of values to be transformed.\n",
    "        vocabulary (Vocabulary object): The vocabulary object.\n",
    "\n",
    "    Returns:\n",
    "        array: The transformed and normalized array of values as integers.\n",
    "    \"\"\"\n",
    "    values = values * vocabulary.max_qty # 1\n",
    "    values = np.where(values < 1, 0, values) # 2\n",
    "    return np.round(values).astype(int) # 3\n",
    "\n",
    "\n",
    "def create_new_routes(df, column_name, vocabulary, cluster_label):\n",
    "    \"\"\"\n",
    "    Creates new routes for each cluster based on the average vector representations in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the cluster data.\n",
    "        column_name (str): The name of the column containing the vector representations of routes.\n",
    "        vocabulary (Vocabulary object): The vocabulary object.\n",
    "        cluster_label (str): The name of the column containing the cluster labels.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a new route for a cluster.\n",
    "\n",
    "    The function iterates over each cluster, calculates the mean vector for the routes in that cluster.\n",
    "    For each mean vector constructs a new route by identifying the most significant city pairs and merchandise for each trip rapresented in the mean vector.\n",
    "    The merchandise vector's part is decoded using transform_values() function above.\n",
    "    \"\"\"\n",
    "    new_routes = []\n",
    "    \n",
    "    city_encoded_length = len(vocabulary.city2id)\n",
    "    merch_encoded_length = len(vocabulary.merchandise2id)\n",
    "    trip_vector_len = city_encoded_length + merch_encoded_length\n",
    "    max_trip = vocabulary.max_trip\n",
    "\n",
    "    # Iterate over each unique cluster\n",
    "    for cluster in df[cluster_label].unique():\n",
    "        # Calculate the mean vector for the routes in the cluster\n",
    "        mean_vector = df[df[cluster_label] == cluster][column_name].mean()\n",
    "        trips = []\n",
    "        previous_start_index = -1\n",
    "\n",
    "        # Construct each trip in the route\n",
    "        for trip in range(max_trip):\n",
    "            low_limit = trip * trip_vector_len\n",
    "            trip_upper_limit = (trip + 1) * trip_vector_len\n",
    "\n",
    "            # Initialize variables to find starting and ending cities\n",
    "            city_start_max_v = 0\n",
    "            city_start_max_v_index = 0 if previous_start_index == -1 else previous_start_index\n",
    "            city_end_max_v = 0\n",
    "            city_end_max_v_index = 0\n",
    "\n",
    "            # Iterate over city segment of the mean vector\n",
    "            for i, cell in enumerate(mean_vector[low_limit : trip_upper_limit - merch_encoded_length]):\n",
    "                if cell != 0 and i < city_encoded_length:\n",
    "                    if previous_start_index == -1 and cell > city_start_max_v:\n",
    "                        city_start_max_v = cell\n",
    "                        city_start_max_v_index = i\n",
    "                    elif cell > city_end_max_v:\n",
    "                        city_end_max_v = cell\n",
    "                        city_end_max_v_index = i\n",
    "\n",
    "            # Break if the start and end cities are the same\n",
    "            if city_start_max_v_index == city_end_max_v_index:\n",
    "                break\n",
    "\n",
    "            # Process merchandise segment of the mean vector\n",
    "            merch_decoded_vector = transform_values(mean_vector[trip_upper_limit - merch_encoded_length  : trip_upper_limit], vocabulary)\n",
    "            merchandise = {}\n",
    "            for i, cell in enumerate(merch_decoded_vector):\n",
    "                if cell != 0:\n",
    "                    merchandise[vocabulary.id2merchandise[i]] = cell\n",
    "\n",
    "            # Create a trip if merchandise is found\n",
    "            if merchandise:\n",
    "                trip = {\n",
    "                    'from' : vocabulary.id2city.get(city_start_max_v_index),\n",
    "                    'to' : vocabulary.id2city.get(city_end_max_v_index),\n",
    "                    'merchandise' : merchandise\n",
    "                }\n",
    "                previous_start_index = city_end_max_v_index\n",
    "                trips.append(trip)\n",
    "\n",
    "        # Add the constructed route for the cluster\n",
    "        new_routes.append({\n",
    "            'id' : f's{cluster}',\n",
    "            'route': trips\n",
    "        })\n",
    "\n",
    "    return new_routes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver favourite routes - Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drivers_favourite_routes(merged_df, key_field, top_n_routes=10):\n",
    "    \"\"\"\n",
    "    Computes and lists the favorite routes of each driver based on merged route data.\n",
    "\n",
    "    Args:\n",
    "        merged_df (pandas.DataFrame): DataFrame containing merged actual routes with the standard assigned.\n",
    "        key_field (str): The name of the field in the DataFrame that holds the similarity score.\n",
    "        top_n_routes (int): The number of top routes to list for each driver.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame where each row represents a driver and includes their average similarity score, \n",
    "                          the number of routes assigned and their top N routes based on preference scores.\n",
    "\n",
    "    The function processes the merged route data to calculate the average similarity for each driver and identifies their \n",
    "    top N routes based on a calculated preference score. The preference score is a product of the average similarity and \n",
    "    the normalized count (frequency) of each standard route being assigned to the driver.\n",
    "    \"\"\"\n",
    "    unique_drivers = merged_df['driver'].unique()\n",
    "    driver_profiles = []\n",
    "\n",
    "    for driver in sorted(unique_drivers):\n",
    "        driver_df = merged_df[merged_df['driver'] == driver]\n",
    "\n",
    "        # Calculate the average similarity score for each driver\n",
    "        average_similarity = driver_df[key_field].mean()\n",
    "        \n",
    "        # Aggregate statistics for each standard route for the driver\n",
    "        std_route_stats = driver_df.groupby('sroute_id')[key_field].agg(['mean', 'count'])\n",
    "\n",
    "        # Normalize the count by the maximum count to get a relative frequency\n",
    "        max_count = std_route_stats['count'].max()\n",
    "        std_route_stats['normalized_count'] = std_route_stats['count'] / max_count\n",
    "        \n",
    "        # Calculate the preference score for each route\n",
    "        std_route_stats['preference_score'] = std_route_stats['mean'] * std_route_stats['normalized_count']\n",
    "\n",
    "        # Get the top N routes based on preference score\n",
    "        top_routes = std_route_stats.sort_values(by='preference_score', ascending=False)[:top_n_routes]\n",
    "\n",
    "        # Append the driver's profile including their average similarity and top N routes\n",
    "        driver_profiles.append({\n",
    "            'driver': driver, \n",
    "            'Routes assigned': len(std_route_stats),\n",
    "            'average_similarity': average_similarity, \n",
    "            'top_routes': top_routes\n",
    "        })\n",
    "\n",
    "    # Return a DataFrame of all driver profiles, indexed by driver\n",
    "    return pd.DataFrame(driver_profiles).set_index('driver')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route generation with collaborative filtering - Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimal_route(matrix, vocabulary):\n",
    "    \"\"\"\n",
    "    Generates optimal routes for each driver based on a rating matrix and vocabulary using a greedy approach.\n",
    "\n",
    "    Args:\n",
    "        matrix (numpy.ndarray): A matrix where each element represents a rating for a driver-trip pair.\n",
    "        vocabulary (Vocabulary object): An object containing mapping and trip information.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where each key is a driver index and the value is a route with the optimal trips.\n",
    "\n",
    "    This function iterates over each driver and builds an optimal route by greedily selecting the best available trip at each step.\n",
    "    The route construction stops if the maximum trip length is reached.\n",
    "    \"\"\"\n",
    "\n",
    "    max_trip_length = vocabulary.max_trip\n",
    "    num_drivers = matrix.shape[0]\n",
    "    optimal_routes = {}\n",
    "\n",
    "    for driver in range(num_drivers):\n",
    "        current_city = None\n",
    "        route = []\n",
    "        trip_indices = list(range(matrix.shape[1]))\n",
    "\n",
    "        while len(route) < max_trip_length:\n",
    "            best_trip = None\n",
    "            best_rating = -1\n",
    "\n",
    "            for trip_index in trip_indices:\n",
    "                trip_values = vocabulary.id2trip.get(trip_index)\n",
    "                start_city, end_city = trip_values[0], trip_values[1]\n",
    "\n",
    "                if current_city is None or current_city == start_city:\n",
    "                    trip_rating = matrix[driver, trip_index]\n",
    "                    if trip_rating > best_rating:\n",
    "                        best_rating = trip_rating\n",
    "                        best_trip = trip_index\n",
    "\n",
    "            if best_trip is not None:\n",
    "                trip_indices.remove(best_trip)\n",
    "                route.append(vocabulary.id2trip.get(best_trip))\n",
    "                current_city = vocabulary.id2trip.get(best_trip)[1]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        optimal_routes[driver] = route\n",
    "\n",
    "    return optimal_routes\n",
    "\n",
    "def convert_routes_to_readable(optimal_routes, vocabulary):\n",
    "    \"\"\"\n",
    "    Converts optimal routes into the requested format.\n",
    "\n",
    "    Args:\n",
    "        optimal_routes (dict): Dictionary containing optimal routes for each driver.\n",
    "        vocabulary (Vocabulary object): An object containing mapping and trip information.\n",
    "\n",
    "    Returns:\n",
    "        list: A list where each element is a dictionary representing a driver's route in a readable format.\n",
    "    \"\"\"\n",
    "    readable_routes = []\n",
    "    for driver, route in optimal_routes.items():\n",
    "        new_route = {'driver': vocabulary.id2driver[driver], 'route': []}\n",
    "\n",
    "        for trip in route:\n",
    "            new_trip = {'from': trip[0], 'to': trip[1], 'merchandise': {merch: qty for merch, qty in trip[2:]}}\n",
    "            new_route['route'].append(new_trip)\n",
    "\n",
    "        readable_routes.append(new_route)\n",
    "    return readable_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Load datasets and components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary & Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = '20_standard_1k_variations' # 50_standard_5k_variations, 50_standard_10k_variations, 20_standard_1k_variations, 20_standard_5k_variations, 20_standard_10k_variations\n",
    "STANDARD_ROUTES_PATH = f'data/{DATASET_SIZE}/standard.json'\n",
    "ACTUAL_ROUTES_PATH = f'data/{DATASET_SIZE}/actual.json'\n",
    "N_PERMUTATIONS = 256\n",
    "\n",
    "vocabulary = Vocabulary(STANDARD_ROUTES_PATH, ACTUAL_ROUTES_PATH)\n",
    "encoder = Encoder(vocabulary, N_PERMUTATIONS)\n",
    "\n",
    "print(f' - Vocabulary info:')\n",
    "print(f' - Max number of consecutive trips: {vocabulary.max_trip}')\n",
    "print(f' - Max quantity of a merch: {vocabulary.max_qty}')\n",
    "print(f' - Min quantity of a merch: {vocabulary.min_qty}')\n",
    "print(f'\\n - Vocabularies print format (element, id)')\n",
    "print(f' - {len(vocabulary.city2id)} cities - Example: {list(vocabulary.city2id.items())[1]}')\n",
    "print(f' - {len(vocabulary.merchandise2id)} merchandise types - Example: {list(vocabulary.merchandise2id.items())[1]}')\n",
    "print(f' - {len(vocabulary.trip2id)} Trips - Example: {list(vocabulary.trip2id.items())[1]}')\n",
    "print(f' - {len(vocabulary.trip_city_pair2id)} trip city pair ids - Example: {list(vocabulary.trip_city_pair2id.items())[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard routes & actual routes\n",
    "\n",
    "Loading standard and actual routes from files and the build a dataframe with:\n",
    "- Matrix of the route - Matrix rapresentation of each route (shape Max trip X cities + merchandise_types)\n",
    "- Vector of the route (flatten matrix)\n",
    "- Vocabulary encoding of the route, cities and merch are translated to ids.\n",
    "- Raw route and raw list of trips\n",
    "- List of trip hashable tuples\n",
    "- List of cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_routes = load_routes(STANDARD_ROUTES_PATH, vocabulary, encoder, 'standard')\n",
    "actual_routes = load_routes(ACTUAL_ROUTES_PATH, vocabulary, encoder, 'actual')\n",
    "\n",
    "merged_routes = pd.merge(actual_routes, standard_routes, on='sroute_id', how='inner', suffixes=( '_act', '_std'))\n",
    "\n",
    "example_row = merged_routes[:2]\n",
    "printTable(example_row[['aroute_id','sroute_id','driver','route_act','route_std','cities_act','cities_std']], 'First two rows of merged routes:\\n')\n",
    "printTable(example_row[['aroute_id','sroute_id','driver','trips_act','trips_std']], 'First two rows of merged routes:\\n')\n",
    "printTable(example_row[['aroute_id','sroute_id','driver','encoded_route_act','encoded_route_std']], 'First two rows of merged routes:\\n')\n",
    "printTable(example_row[['aroute_id','sroute_id','driver','matrix_act','matrix_std']], 'First two rows of merged routes:\\n')\n",
    "printTable(example_row[['aroute_id','sroute_id','driver','vector_act','vector_std']], 'First two rows of merged routes:\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Utility matrix and collaborative trip filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level 1 utility matrix\n",
    "\n",
    "Generate the utility matrix at trip level. A single trip performed by a specific driver, as a matrix, is evaluated as follow:\n",
    "- Examine an actual route matrix with the respective assigned standard route matrix.\n",
    "- Process at trip level (row) the actual route.\n",
    "- For each tripA (row) of actual route matrix consider each tripS (row) in the standard route matrix.\n",
    "- (A) If tripA has an equal vector in the same position of the tripS, this is a complete match. (rating = 1)\n",
    "- (B) Otherwise, a metric is used to compuet the similarity between tripA and each, non-zero (requested), tripS in the standard route matrix, this value is added to the summation matrix.\n",
    "- (B) Then the number of counts, 1 in case (A), k in case (B), is saved in another matrix, the count matrix.\n",
    "\n",
    "This process build two matrices: \n",
    "- one with the summation of similarity of each not correct trip with each trip requested in the correspondant standard route, 1 only for correct trips.\n",
    "- one with the count of the comparison made for each trip.\n",
    "\n",
    "The utility matrix is at the end computed as the summation matrix divided by the count matrix. This give the ratings of each performed trip for each driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = 'vector'\n",
    "utility_matrix = UtilityMatrix(merged_routes, KEY, vocabulary)\n",
    "\n",
    "print(f'Utility matrix level 1 loaded.')\n",
    "print(f' - Shape: {utility_matrix.matrix_lv1.shape}')\n",
    "print(f' - Max value: {utility_matrix.matrix_lv1.max()}')\n",
    "print(f' - Min value: {utility_matrix.matrix_lv1.min()}')\n",
    "print(f' - Mean value: {utility_matrix.matrix_lv1.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate driver profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = DriversProfile(utility_matrix.matrix_lv1, vocabulary)\n",
    "\n",
    "print(f'Driver profiles generated from level 1 utility matrix.')\n",
    "print(f' - Shape: {profiles.drivers_profile.shape}')\n",
    "print(f' - Max value: {profiles.drivers_profile.max()}')\n",
    "print(f' - Min value: {profiles.drivers_profile.min()}')\n",
    "print(f' - Mean value: {profiles.drivers_profile.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level 2 utility matrix - Collaborative filetring predictions with driver profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the value of k for drivers and trips\n",
    "k_drivers = 100\n",
    "\n",
    "# Compute top-k similar drivers\n",
    "top_k_drivers = get_top_k_similar_indices(profiles.drivers_profile, k_drivers)\n",
    "\n",
    "utility_matrix.predict_ratings_with_driver_profile_top_k(profiles.drivers_profile, top_k_drivers)\n",
    "\n",
    "print(f'Utility matrix level 1 updated using collaborative filtering -> Level 2 matrix')\n",
    "print(f' - Shape: {utility_matrix.matrix_lv2.shape}')\n",
    "print(f' - Max value: {utility_matrix.matrix_lv2.max()}')\n",
    "print(f' - Min value: {utility_matrix.matrix_lv2.min()}')\n",
    "print(f' - Mean value: {utility_matrix.matrix_lv2.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(utility_matrix.matrix_lv1, max_x=50, max_y=50, ylabel='Driver', xlabel='Trip', title='Utility matrix (Drivers x Trips) (Limited to 50x50)')\n",
    "plot_matrix(profiles.drivers_profile, max_x=50, max_y=50, ylabel='Driver', xlabel='Trip', title='Drivers profiles(Drivers x Trips) (Limited to 50x50)')\n",
    "plot_matrix(utility_matrix.matrix_lv2, max_x=50, max_y=50, ylabel='Driver', xlabel='Trip', title='Level 2 utility matrix with driver profile (Drivers x Trips) (Limited to 50x50)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trip ranking & trip rating feature\n",
    "\n",
    "Using the previously computed utility matrix, here is computed a score for each trip as follow:\n",
    "- Summed the non-zero values in each column (trip) of the utility matrix.\n",
    "- Count of the non-zero trips for each column.\n",
    "- (A) Compute the average of these scores.\n",
    "- Get the total number of times that each trip has been executed\n",
    "- (B) Normalize the previous step counts dividing by the maximum\n",
    "- Compute the normalized score multipling the score at (A) by the normalized count at (B)\n",
    "\n",
    "These final scores are then concatenated at the end of each correspondant row (trip) of the matrix of each route. \n",
    "This result in an increase of the dimensionality of 1.\n",
    "\n",
    "The new matrices and vectors are inserted in two new columsn of the dataframe (actual route and standard routes):\n",
    "- vector -> vector_f1\n",
    "- matrix -> matrix_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_ranking = trip_evaluation(utility_matrix.matrix_lv2, utility_matrix.trip_count_matrix, vocabulary)\n",
    "\n",
    "add_trip_level_feature(trip_ranking, actual_routes, vocabulary, feature_name = 'normalized_score')\n",
    "add_trip_level_feature(trip_ranking, standard_routes, vocabulary, feature_name = 'normalized_score')\n",
    "\n",
    "printTable(trip_ranking, 'Top 10 trip ratings')\n",
    "printTable(actual_routes[['matrix','matrix_f1']][:2], '\\nRoute encode without and with the trip rating feature.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector principal component analysis (PCA)\n",
    "\n",
    "The followign trasformation will modifiy and add new columns to the standard routes and actual routes dataframes as follow:\n",
    "\n",
    "- vector    -> vector_pca \n",
    "- vector_f1 -> vector_f1_pca\n",
    "\n",
    "The left field is transformed with PCA to a reduced dimensionality for clustering analysis. The chosen dimensionality is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_COMPONENTS = len(vocabulary.stdroutes)\n",
    "\n",
    "# Generate vector_f1_pca\n",
    "encoder.apply_PCA(actual_routes, 'vector_f1', PCA_COMPONENTS)\n",
    "encoder.apply_PCA(standard_routes, 'vector_f1', PCA_COMPONENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Routes in 2D and 3D\n",
    "\n",
    "For plotting, the input vectors are reduced using PCA to be in 2D and 3D, if the vector shape doesn't match these two dimensionality constraints.\n",
    "\n",
    "For example, this cause that for the normalized vector with PCA already applied, if the number of components is different from 3, the PCA is applied again over the previous PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_routes(actual_routes, 'vector')\n",
    "plot_routes(actual_routes, 'vector_f1')\n",
    "plot_routes(actual_routes, 'vector_f1_pca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of clusters analysis - Results in /data/cluster_number_analysis/\n",
    "\n",
    "This analysis will evaluate the numbers of cluster in the specified range ([x,y,step]).\n",
    "\n",
    "Clusters are evaluated using the inertia, Silhouette score and Davis Bouldin score. The evaluation is performed on a basic configuration of the two model adopted: KMeans and KMedoids.\n",
    "\n",
    "CLUSTERING_KEY define the type of encoding of the route to use for the clustering phase, or in other word the name of the column of the dataframe to use, which are:\n",
    "\n",
    "- vector\n",
    "\n",
    "- vector_f1 \n",
    "\n",
    "- vector_f1_pca\n",
    "\n",
    "This analysis was conducted for each available CLUSTERING_KEY. The rapresentation that yields best results in terms of silhouette score and David Bouldin score is the one that uses vector_f1_pca.\n",
    "\n",
    " The range analyzed is [2,100] with a step of 2 clusters. The charts are saved inside \"cluster_number_analysis\" folder in the same directory of this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_clusters(actual_routes, 'vector_f1_pca', 'kmeans', f'data/cluster_number_analysis/{DATASET_SIZE}/kmeans_clustering_range21002.png', 2, 100, 2) \n",
    "find_optimal_clusters(actual_routes, 'vector_f1_pca', 'kmedoids', f'data/cluster_number_analysis/{DATASET_SIZE}/kmedoids_clustering_range21002.png', 2, 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search for clustering methods hyperparameters\n",
    "\n",
    "With this grid search the goal is to find the best hyperparameters configuration for the two adopted clustering models (KMeans and KMedoids). \n",
    "\n",
    "It's also possible to specify different cluster numbers but they should be setted considering the previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common property\n",
    "cluster_sizes = [50]\n",
    "CLUSTERING_KEY = 'vector_f1_pca'\n",
    "\n",
    "# KMEAN\n",
    "kmeans_parameter_combinations = [\n",
    "    \n",
    "    {'n_clusters': n_clusters, 'tol': tol, 'n_init':n_init, 'init': init, 'algorithm':algorithm, 'max_iter':max_iter}\n",
    "\n",
    "    for n_clusters in cluster_sizes\n",
    "    for tol in [1e-4, 1e-5, 1e-3] # default: 1e-4\n",
    "    for n_init in [10] # default:10\n",
    "    for init in ['k-means++'] # default:k-means++\n",
    "    for algorithm in ['lloyd'] # default:lloyd - elkan\n",
    "    for max_iter in [300, 600, 900] # default:300\n",
    "]\n",
    "\n",
    "best_score_kmeans, best_config_kmeans = grid_search(kmeans_parameter_combinations, actual_routes, CLUSTERING_KEY, 'kmeans', plot_enabled = False)\n",
    "\n",
    "# KMEDOIDS\n",
    "kmedoids_parameter_combinations = [\n",
    "    \n",
    "    {'n_clusters': n_clusters,  'init':init,  'metric': metric, 'method':method, 'max_iter':max_iter}\n",
    "\n",
    "    for n_clusters in cluster_sizes\n",
    "    for init in ['k-medoids++','random','build'] # default:heuristic - random - build\n",
    "    for metric in ['cosine'] # default:euclidean\n",
    "    for method in ['alternate'] # default:alternate - pam (has no end)\n",
    "    for max_iter in [300] # default:300\n",
    "]\n",
    "\n",
    "best_score_kmedoids, best_config_kmedoids = grid_search(kmedoids_parameter_combinations, actual_routes, CLUSTERING_KEY, 'kmedoids', plot_enabled = False)\n",
    "\n",
    "# Best configurations\n",
    "print(f'\\n\\nGrid search analysis for KMeans\\nBest configuration:\\n - Silhouette score: {best_score_kmeans[1]}\\n - Bouldin score: {best_score_kmeans[2]}\\n - Hyperparameters:')\n",
    "for key, el in best_config_kmeans.items():\n",
    "    print(f' -- {key} : {el}')\n",
    "\n",
    "print(f'\\n\\nGrid search analysis for KMedoids\\nBest configuration:\\n - Silhouette score: {best_score_kmedoids[1]}\\n - Bouldin score: {best_score_kmedoids[2]}\\n - Hyperparameters:')\n",
    "for key, el in best_config_kmedoids.items():\n",
    "    print(f' -- {key} : {el}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single configuration clustering\n",
    "\n",
    "This is the code with fixed parameter for the two adopted clustering models (KMedoids and KMeans). \n",
    "\n",
    "The labels of the clusters are inserted into the appropriate column of the actual routes dataframe. Thus, each actual route has a cluster assigned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERING_KEY = 'vector_f1_pca'\n",
    "\n",
    "#KMEAN_CONFIGURATION = best_config_kmeans\n",
    "KMEAN_CONFIGURATION = {'n_clusters': 50, 'init': 'k-means++', 'tol': 1e-4, 'algorithm':'lloyd', 'max_iter':300}\n",
    "\n",
    "#KMEDOIDS_CONFIGURATION = best_config_kmedoids\n",
    "KMEDOIDS_CONFIGURATION = {'n_clusters': 50, 'init': 'build', 'metric':'cosine', 'method':'alternate', 'max_iter':300}\n",
    "\n",
    "kmeans_cluster_model, kmeans_silhouette, kmeans_davies_bouldin = cluster_and_evaluate(actual_routes, KMEAN_CONFIGURATION, CLUSTERING_KEY, approach='kmeans')\n",
    "kmedoids_cluster_model, kmedoids_silhouette, kmedoids_davies_bouldin = cluster_and_evaluate(actual_routes, KMEDOIDS_CONFIGURATION, CLUSTERING_KEY, approach='kmedoids')\n",
    "\n",
    "print(f'\\nKMeans Parameters: {KMEAN_CONFIGURATION}')\n",
    "print(f'Silhouette score: {kmeans_silhouette}')\n",
    "print(f'Davies Bouldin score: {kmeans_davies_bouldin}')\n",
    "\n",
    "print(f'\\nKMedoids Parameters: {KMEDOIDS_CONFIGURATION}')\n",
    "print(f'Silhouette score: {kmedoids_silhouette}')\n",
    "print(f'Davies Bouldin score: {kmedoids_davies_bouldin}')\n",
    "\n",
    "clustering_summary(actual_routes, cluster_column='cluster_kmeans')\n",
    "clustering_summary(actual_routes, cluster_column='cluster_kmedoids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clusters in 2D and 3D\n",
    "\n",
    "(For plotting, the input vectors are reduced using PCA to be in 2D and 3D, if the vector shape doesn't match these two dimensionality constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vector_elements(actual_routes, CLUSTERING_KEY, 'cluster_kmeans')\n",
    "plot_vector_elements(actual_routes, CLUSTERING_KEY, 'cluster_kmedoids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Clustering - Generate new standard route from centroids and medoids (Project task 1)\n",
    "\n",
    "The new standard route with medoids corresponds to some actual routes.\n",
    "\n",
    "The new standard routes with centroid are generated with the logic descibed in the function create_new_routes()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REC_STANDARD_PATH = f'results/{DATASET_SIZE}/recStandard.json'\n",
    "\n",
    "recStandard_from_mean = create_new_routes(actual_routes, 'vector', vocabulary, 'cluster_kmeans')\n",
    "medoid = actual_routes.iloc[kmedoids_cluster_model.medoid_indices_]\n",
    "recStandard_from_medoid = [{ 'id':f's{i}', 'route':route } for i, route in enumerate(medoid['route'])]\n",
    "\n",
    "save_new_standard_routes(recStandard_from_mean + recStandard_from_medoid, REC_STANDARD_PATH)\n",
    "\n",
    "printTable(medoid[['sroute_id','aroute_id','route']], 'Medoids:')\n",
    "print(f'{r}\\n' for r in recStandard_from_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Collaborative filtering - Generating new standard routes using the level 2 utility matrix (Project task 3)\n",
    "\n",
    "Here are generated the ideal route for each driver. This route is build using the trip ratings in the level 2 utility matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMAL_DRIVERS_ROUTES_PATH = f'results/{DATASET_SIZE}/perfectRoute.json'\n",
    "optimal_routes = generate_optimal_route(utility_matrix.matrix_lv2, vocabulary)\n",
    "optimal_routes_encoded = convert_routes_to_readable(optimal_routes, vocabulary)\n",
    "save_new_standard_routes(optimal_routes_encoded, OPTIMAL_DRIVERS_ROUTES_PATH)\n",
    "\n",
    "for opt_route in optimal_routes_encoded:\n",
    "    print(opt_route)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Drivers best routes - Project task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute similarity scores of the merged routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_standard_routes = load_routes(REC_STANDARD_PATH, vocabulary, encoder, 'standard')\n",
    "old_standard_routes = load_routes(STANDARD_ROUTES_PATH, vocabulary, encoder, 'standard')\n",
    "new_standard_routes = pd.concat([rec_standard_routes, old_standard_routes], axis = 0, ignore_index = True)\n",
    "\n",
    "merged_routes['jaccard_similarity'] = merged_routes.apply(lambda row: jaccard_similarity(row['signature_act'], row['signature_std']), axis=1)\n",
    "merged_routes['cosine_similarity'] = merged_routes.apply(lambda row: cosine_similarity(row['vector_act'].reshape(1, -1), row['vector_std'].reshape(1, -1))[0,0], axis=1)\n",
    "merged_routes['similarity'] = (merged_routes['jaccard_similarity'] + merged_routes['cosine_similarity']) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the computed similarity score to analyze the driver route divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PREFERENCES_PATH = f'results/{DATASET_SIZE}/driver.json'\n",
    "driver_preferences = get_drivers_favourite_routes(merged_routes, 'similarity', top_n_routes=10)\n",
    "save_top_routes_to_json(driver_preferences, DRIVER_PREFERENCES_PATH)\n",
    "\n",
    "printTable(driver_preferences, 'Driver least divergence standard routes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drivers top-5 routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers_top_5_routes = print_driver_top_routes_from_json(DRIVER_PREFERENCES_PATH, standard_routes)\n",
    "print(drivers_top_5_routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Evaluation\n",
    "\n",
    "To try to inspect the quality of the new standard routes with respect to the old one are used similarity matrices.\n",
    "They are computed using the dataframe field specified in CLUSTERING_KEY for the cosine similarity matrix and the field signature for jaccard similarity matrices.\n",
    "\n",
    "The scope is to observe how the new standard routes are distributed with respect to the actual and the old standard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old standard routes evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_KEY = 'vector'\n",
    "\n",
    "jaccard_sm_std_act = calculate_similarity_matrix(standard_routes[:4], actual_routes[:4], 'signature', 'jaccard', f'../data/matrices/{DATASET_SIZE}/jaccard_sm_std_act.npy')\n",
    "cosine_sm_std_act = calculate_similarity_matrix(standard_routes, actual_routes, EVALUATION_KEY, 'cosine', f'../data/matrices/{DATASET_SIZE}/cosine_sm_std_act.npy')\n",
    "\n",
    "print(f'\\nOld standard vs Actual - Jaccard similarity matrix:\\n - Shape: {jaccard_sm_std_act.shape}\\n - Mean: {jaccard_sm_std_act.mean().round(4)}\\n - Std: {jaccard_sm_std_act.std().round(4)}')\n",
    "print(f'\\nOld standard vs Actual  - Cosine similarity matrix:\\n - Shape: {cosine_sm_std_act.shape}\\n - Mean: {cosine_sm_std_act.mean().round(4)}\\n - Std: {cosine_sm_std_act.std().round(4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New standard routes evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New standard vs Actual\n",
    "jaccard_sm_recstd_act = calculate_similarity_matrix(rec_standard_routes, actual_routes, 'signature', 'jaccard', f'../data/matrices/{DATASET_SIZE}/jaccard_sm_recstd_act.npy')\n",
    "cosine_sm_recstd_act = calculate_similarity_matrix(rec_standard_routes, actual_routes, EVALUATION_KEY, 'cosine', f'../data/matrices/{DATASET_SIZE}/cosine_sm_recstd_act.npy')\n",
    "\n",
    "# New standard vs Actual\n",
    "print(f'\\nNew Standard vs Actual  - Jaccard similarity matrix:\\n - Shape: {jaccard_sm_recstd_act.shape}\\n - Mean: {jaccard_sm_recstd_act.mean().round(4)}\\n - Std: {jaccard_sm_recstd_act.std().round(4)}')\n",
    "print(f'\\nNew Standard vs Actual - Cosine similarity matrix:\\n - Shape: {cosine_sm_recstd_act.shape}\\n - Mean: {cosine_sm_recstd_act.mean().round(4)}\\n - Std: {cosine_sm_recstd_act.std().round(4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot evaluation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old standard vs Actual\n",
    "plot_matrix(jaccard_sm_std_act, max_x= 25, max_y=25, xlabel= 'Standard routes', ylabel= 'Actual routes', title = 'Jaccard similarity matrix - OLD Standard vs Actual')\n",
    "plot_matrix(cosine_sm_std_act, max_x= 25, max_y=25, xlabel= 'Standard routes', ylabel= 'Actual routes', title = 'Cosine similarity matrix - OLD Standard vs Actual')\n",
    "\n",
    "# New standard vs Actual\n",
    "plot_matrix(jaccard_sm_recstd_act, max_x= 25, max_y=25, xlabel= 'Rec Standard routes', ylabel= 'Actual routes', title = 'Jaccard similarity matrix - New Standard vs Actual')\n",
    "plot_matrix(cosine_sm_recstd_act, max_x= 25, max_y=25, xlabel= 'Rec Standard routes', ylabel= 'Actual routes', title = 'Cosine similarity matrix - New Standard vs Actual')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of top-5 identified favourite routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(location='results/20_standard_3k_variations/driver.json'):\n",
    "    \n",
    "    with open(location) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return {item['driver']: item['routes'] for item in data}\n",
    "\n",
    "def calculate_similarity(df):\n",
    "    \n",
    "    return df.apply(lambda row: jaccard_similarity(np.array(row['signature_act']), np.array(row['signature_std'])), axis=1).mean()\n",
    "\n",
    "def process_driver_data(merged_routes, driver_prefix, driver_fav_routes):\n",
    "    \n",
    "    all_df = merged_routes[merged_routes['driver'] == f'{driver_prefix}']\n",
    "    fav_df = merged_routes[(merged_routes['driver'] == f'{driver_prefix}') & (merged_routes['sroute_id'].isin(driver_fav_routes))]\n",
    "    non_fav_df = merged_routes[(merged_routes['driver'] == f'{driver_prefix}') & (~merged_routes['sroute_id'].isin(driver_fav_routes))]\n",
    "    \n",
    "    a = calculate_similarity(fav_df)\n",
    "    b = calculate_similarity(all_df)\n",
    "    c = calculate_similarity(non_fav_df)\n",
    "    \n",
    "    return a, b, c\n",
    "\n",
    "# Load JSON data\n",
    "data_dict = load_json()\n",
    "\n",
    "# Process data for each driver\n",
    "fav_route_similarity_scores, all_routes_similarity_scores, non_fav_route_similarity_scores = zip(*[process_driver_data(merged_routes, f'driver_{i + 1}', data_dict[f'driver_{i+1}']) for i in range(len(data_dict.keys()))])\n",
    "\n",
    "# Concatenate arrays into a single DataFrame\n",
    "columns = ['Only favourite routes', 'All routes', 'Routes without favourites']\n",
    "driver_values = pd.DataFrame(list(zip(fav_route_similarity_scores, all_routes_similarity_scores, non_fav_route_similarity_scores)), columns=columns, index=[f'driver_{i + 1}' for i in range(len(data_dict.keys()))])\n",
    "\n",
    "# Plotting\n",
    "plot_similarity(driver_values)\n",
    "driver_values.describe() # the statistics\n",
    "driver_values.mean() # aggregated mean, showing the overall increase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary info\n",
    "\n",
    "print(f' - Vocabulary max qty:', vocabulary.max_qty)\n",
    "print(f' - Dataset, loaded {len(standard_routes)} standard routes.')\n",
    "print(f' - Dataset, loaded {len(actual_routes)} actual routes.')\n",
    "print(f' - Dataset, loaded {len(merged_routes)} merged routes.')\n",
    "print(f' - Vector len: ', len(actual_routes['vector'][0]))\n",
    "print(f' - Matrix shape: ', actual_routes['matrix'][0].shape)\n",
    "print(f' - Signature len: ', len(actual_routes['signature'][0]))\n",
    "\n",
    "print(f'\\nDataframes structure (standard and actual dataframes have same structure):\\n')\n",
    "print(standard_routes.info(), '\\n\\n')\n",
    "print(merged_routes.info(), '\\n\\n')\n",
    "\n",
    "# Actual routes\n",
    "print('\\nActual routes examples:')\n",
    "printTable(actual_routes[['aroute_id', 'route']][2:3], 'Route:')\n",
    "printTable(actual_routes[['aroute_id', 'cities']][2:3], 'Cities:')\n",
    "printTable(actual_routes[['aroute_id', 'trips']][2:3], 'Trips:')\n",
    "printTable(actual_routes[['aroute_id', 'encoded_route']][2:3], 'Encoded route:')\n",
    "printTable(actual_routes[['aroute_id', 'matrix']][2:3], 'Route matrix:')\n",
    "printTable(actual_routes[['aroute_id', 'matrix_f1']][2:3], 'Route matrix:')\n",
    "printTable(actual_routes[['aroute_id', 'signature']][2:3], 'Signature:')\n",
    "\n",
    "\n",
    "\n",
    "print(f'jaccard_similarity   - MAX:',merged_routes['jaccard_similarity'].max(),' - MIN:', merged_routes['jaccard_similarity'].min(), ' - MEAN:',merged_routes['jaccard_similarity'].mean())\n",
    "print(f'cosine_similarity    - MAX:',merged_routes['cosine_similarity'].max(),' - MIN:', merged_routes['cosine_similarity'].min(), ' - MEAN:',merged_routes['cosine_similarity'].mean())\n",
    "print(f'euclidean_similarity - MAX:',merged_routes['euclidean_similarity'].max(),' - MIN:', merged_routes['euclidean_similarity'].min(), ' - MEAN:',merged_routes['euclidean_similarity'].mean())\n",
    "\n",
    "printTable(merged_routes[[\n",
    "    'aroute_id', 'sroute_id','cities_std','cities_act',\n",
    "    'jaccard_similarity',\n",
    "    'cosine_similarity',\n",
    "    'euclidean_similarity',\n",
    "]][:100], 'Similarities:', limit=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
